{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. 본격적으로 Numpy와 친해지기 위해서 다양한 연산을 연습해볼 예정입니다. 무작위의 데이터를 가 진 5x3의 행렬을 가지는 numpy array와 3x2 행렬을 가지는 numpy array를 만든 후 행열곱 연산을 진행해보세요.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.18874792, 0.6955307 ],\n",
       "        [0.62618106, 0.36683106],\n",
       "        [0.43266168, 0.27924056],\n",
       "        [1.19985334, 0.68060961],\n",
       "        [0.44227809, 0.2926193 ]]),\n",
       " (5, 2))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.random.random_sample(15).reshape(5,3)\n",
    "arr2 = np.random.random_sample(6).reshape(3,2)\n",
    "arr1, arr2\n",
    "dot = arr1.dot(arr2)\n",
    "\n",
    "dot, dot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. 두번째로는 numpy에서 자주 사용하는 concatenate 연산에 대한 미션을 수행해보겠습니다. 이제 Numpy에서 사용하는 2개의 numpy array가 있을때, 두 numpy array의 concatenate 연산을 구해보세요.  \n",
    "\n",
    "axis는 0, 1 두개로 연산한 결과를 출력해보세요.\n",
    "각 데이터가 어떤 형태로 더해지는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.,  7.],\n",
       "        [ 9., 11.],\n",
       "        [ 2.,  4.],\n",
       "        [ 6.,  8.]]),\n",
       " array([[ 5.,  7.,  2.,  4.],\n",
       "        [ 9., 11.,  6.,  8.]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = np.array([[5, 7], [9,11]], float) \n",
    "arr2 = np.array([[2, 4], [6,8]], float) \n",
    "\n",
    "concate_1 = np.vstack((arr1, arr2))\n",
    "concate_2 = np.hstack((arr1, arr2))\n",
    "\n",
    "concate_1, concate_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.,  7.],\n",
       "        [ 9., 11.],\n",
       "        [ 2.,  4.],\n",
       "        [ 6.,  8.]]),\n",
       " array([[ 5.,  7.,  2.,  4.],\n",
       "        [ 9., 11.,  6.,  8.]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = np.array([[5, 7], [9,11]], float) \n",
    "arr2 = np.array([[2, 4], [6,8]], float) \n",
    "\n",
    "concate_1 = np.concatenate((arr1, arr2), axis=0)\n",
    "concate_2 = np.concatenate((arr1, arr2), axis=1)\n",
    "\n",
    "concate_1, concate_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. 3번부터 5번까지의 미션는 Numpy를 이용해서 정답값을 예측해보는 linear regression을 구현해 보는 미션입니다. 첫번째 단계로 데이터를 준비해보도록 하겠습니다. 아래와 같이 데이터가 주어져있을 때, 경사하강법을 위한 데이터를 분리해보세요.\n",
    "\n",
    "주어진 xy 데이터를 이용해서 학습과 정답 데이터를 준비해보세요.\n",
    "*추가 수정* 문제에서 주어진 xy에서 대괄호를 한 번 더 묶어주어야 문제 해결이 가능합니다.\n",
    "(차원 관련)  (np.array([[1., 2., 3., 4., 5., 6.], [10., 20., 30., 40., 50., 60.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6.]),\n",
       " (6,),\n",
       " array([10., 20., 30., 40., 50., 60.]),\n",
       " (6,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = (np.array([[1., 2., 3., 4., 5., 6.], [10., 20., 30., 40., 50., 60.]]))\n",
    "\n",
    "x_train = xy[0].transpose()\n",
    "y_train = xy[1].transpose()\n",
    "\n",
    "x_train, x_train.shape, y_train, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. 경사 하강법 구현을 위해서 위에서 분리한 x_train 데이터와 계산될 weight, bias 값을 정의해보세요. 여기서 구현한 weight와 bias 는 linear regression 계산을 진행할시 직선의 기울기와 y 절편의 값이 됩니다.\n",
    "\n",
    "numpy의 random 함수를 이용하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1631342772663218, -0.18116826198159885)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_gd = (np.random.uniform(-1, 1))\n",
    "bias    = (np.random.uniform(-1, 1))\n",
    "beta_gd, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. 이제 최종적으로 linear regression을 경사하강법으로 학습하는 코드를 구현해봅시다. 경사하강법 구현을 위한 학습 Loop를 구현해보세요. 최종적으로 1000회 반복했을 시에 결과를 출력하세요.\n",
    "\n",
    "단, Error는 차이의 제곱을 이용해서 계산해주세요.\n",
    "Gradient 값은 우리가 예측하는 각 변수에 대한 미분값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          0/1000, Error:  71.883342, Beta_gd:   1.209887, bias:  -0.168628\n",
      "Epoch:        100/1000, Error:   0.007503, Beta_gd:   2.953961, bias:   0.197102\n",
      "Epoch:        200/1000, Error:   0.005207, Beta_gd:   2.961646, bias:   0.164200\n",
      "Epoch:        300/1000, Error:   0.003614, Beta_gd:   2.968049, bias:   0.136790\n",
      "Epoch:        400/1000, Error:   0.002508, Beta_gd:   2.973382, bias:   0.113955\n",
      "Epoch:        500/1000, Error:   0.001741, Beta_gd:   2.977826, bias:   0.094933\n",
      "Epoch:        600/1000, Error:   0.001208, Beta_gd:   2.981527, bias:   0.079085\n",
      "Epoch:        700/1000, Error:   0.000838, Beta_gd:   2.984611, bias:   0.065884\n",
      "Epoch:        800/1000, Error:   0.000582, Beta_gd:   2.987180, bias:   0.054886\n",
      "Epoch:        900/1000, Error:   0.000404, Beta_gd:   2.989320, bias:   0.045724\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터 만들기\n",
    "xy = (np.array([[1., 2., 3., 4., 5., 6.], [3., 6., 9., 12., 15., 18.]]))\n",
    "\n",
    "x_train = xy[0].transpose()\n",
    "y_train = xy[1].transpose()\n",
    "\n",
    "beta_gd = np.random.uniform(-1, 1)\n",
    "bias    = np.random.uniform(-1, 1)\n",
    "\n",
    "learning_rate = 0.01 \n",
    "\n",
    "for i in range(1000):\n",
    "    Y_Pred = beta_gd * x_train + bias\n",
    "    err = ((Y_Pred - y_train)**2).mean()\n",
    "    \n",
    "    if err < 0.01 :\n",
    "        break \n",
    "          \n",
    "    #경사하강법 적용 \n",
    "    beta_grad = learning_rate * ((Y_Pred - y_train) * x_train).mean()\n",
    "    bias_grad = learning_rate *  (Y_Pred - y_train).mean()\n",
    "\n",
    "    # beta_gd, bias 값 갱신 \n",
    "    beta_gd = beta_gd - beta_grad\n",
    "    bias    = bias    - bias_grad\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch: {:10d}/1000, Error: {:10f}, Beta_gd: {:10f}, bias: {:10f}'.format(i, err, beta_gd.item(), bias.item()))\n",
    "        Y_Pred = beta_gd * x_train + bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:          0/1000, Error: 166.963131, Beta_gd:   0.027287, bias:   0.802833\n",
      "Epoch:        100/1000, Error:   0.270167, Beta_gd:   2.723736, bias:   1.182740\n",
      "Epoch:        200/1000, Error:   0.187497, Beta_gd:   2.769853, bias:   0.985304\n",
      "Epoch:        300/1000, Error:   0.130124, Beta_gd:   2.808272, bias:   0.820827\n",
      "Epoch:        400/1000, Error:   0.090306, Beta_gd:   2.840277, bias:   0.683805\n",
      "Epoch:        500/1000, Error:   0.062673, Beta_gd:   2.866940, bias:   0.569657\n",
      "Epoch:        600/1000, Error:   0.043495, Beta_gd:   2.889152, bias:   0.474564\n",
      "Epoch:        700/1000, Error:   0.030186, Beta_gd:   2.907656, bias:   0.395344\n",
      "Epoch:        800/1000, Error:   0.020949, Beta_gd:   2.923071, bias:   0.329349\n",
      "Epoch:        900/1000, Error:   0.014539, Beta_gd:   2.935913, bias:   0.274371\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터 만들기\n",
    "xy = (np.array([[1., 2., 3., 4., 5., 6.], [3., 6., 9., 12., 15., 18.]]))\n",
    "\n",
    "x_train = xy[0].transpose()\n",
    "y_train = xy[1].transpose()\n",
    "\n",
    "beta_gd = np.random.uniform(-1, 1)\n",
    "bias    = np.random.uniform(-1, 1)\n",
    "\n",
    "learning_rate = 0.01 \n",
    "\n",
    "for i in range(1000):\n",
    "    Y_pred = beta_gd * x_train + bias\n",
    "    err = ((Y_pred - y_train)**2).mean()\n",
    "          \n",
    "    #경사하강법 적용 \n",
    "    beta_grad = learning_rate * ((Y_pred - y_train) * x_train).mean()\n",
    "    bias_grad = learning_rate *  (Y_pred - y_train).mean()\n",
    "\n",
    "    # beta_gd, bias 값 갱신 \n",
    "    beta_gd = beta_gd - beta_grad\n",
    "    bias    = bias    - bias_grad\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch: {:10d}/1000, Error: {:10f}, Beta_gd: {:10f}, bias: {:10f}'.format(i, err, beta_gd.item(), bias.item()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
